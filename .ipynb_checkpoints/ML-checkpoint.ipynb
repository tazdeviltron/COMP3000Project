{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b597b8b0-98ba-45f9-9196-93bdaa84befc",
   "metadata": {},
   "source": [
    "# **Heart disease prediction model**\n",
    "## Step 1: Load the dataset and Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6bf553e-b994-43ba-821c-083ff73862d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  Sex  Chest pain type   BP  Cholesterol  FBS over 120  EKG results  \\\n",
      "0     70    1                4  130          322             0            2   \n",
      "1     67    0                3  115          564             0            2   \n",
      "2     57    1                2  124          261             0            0   \n",
      "3     64    1                4  128          263             0            0   \n",
      "4     74    0                2  120          269             0            2   \n",
      "..   ...  ...              ...  ...          ...           ...          ...   \n",
      "265   52    1                3  172          199             1            0   \n",
      "266   44    1                2  120          263             0            0   \n",
      "267   56    0                2  140          294             0            2   \n",
      "268   57    1                4  140          192             0            0   \n",
      "269   67    1                4  160          286             0            2   \n",
      "\n",
      "     Max HR  Exercise angina  ST depression  Slope of ST  \\\n",
      "0       109                0            2.4            2   \n",
      "1       160                0            1.6            2   \n",
      "2       141                0            0.3            1   \n",
      "3       105                1            0.2            2   \n",
      "4       121                1            0.2            1   \n",
      "..      ...              ...            ...          ...   \n",
      "265     162                0            0.5            1   \n",
      "266     173                0            0.0            1   \n",
      "267     153                0            1.3            2   \n",
      "268     148                0            0.4            2   \n",
      "269     108                1            1.5            2   \n",
      "\n",
      "     Number of vessels fluro  Thallium Heart Disease  \n",
      "0                          3         3      Presence  \n",
      "1                          0         7       Absence  \n",
      "2                          0         7      Presence  \n",
      "3                          1         7       Absence  \n",
      "4                          1         3       Absence  \n",
      "..                       ...       ...           ...  \n",
      "265                        0         7       Absence  \n",
      "266                        0         7       Absence  \n",
      "267                        0         3       Absence  \n",
      "268                        0         6       Absence  \n",
      "269                        3         3      Presence  \n",
      "\n",
      "[270 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Data preparation\n",
    "# Load the CSV file\n",
    "data = pd.read_csv('Heart_Disease_Prediction.csv', header = 0)\n",
    "\n",
    "# Display\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5969d4-c53a-40dd-837a-1de5200a501c",
   "metadata": {},
   "source": [
    "## Step 2: Train Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "500141d6-d83a-4fcd-9444-ba30ccaf68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)   # Converts 'Absence'/'Presence' â†’ 0/1\n",
    "\n",
    "#split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=0)\n",
    "\n",
    "# Scale the data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "#data for aNN\n",
    "X_train_nn = X_train_scaled.T\n",
    "X_test_nn = X_test_scaled.T\n",
    "y_train_nn = y_train.reshape(1, -1)\n",
    "y_test_nn = y_test.reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf62628-438f-4daa-885d-be74734d44e6",
   "metadata": {},
   "source": [
    "## Step 3: KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f35d208c-6924-40e6-a6cd-ca5b84d1940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier Accuracy: 0.7941176470588235\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "#train data on KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "knn_predictions = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = knn.score(X_test_scaled, y_test)\n",
    "print(f'KNN Classifier Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bfcceb-2316-4570-8fea-e2c8257693dc",
   "metadata": {},
   "source": [
    "## Step4: Rainforest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "774d0623-a65b-400e-94cb-1d9fda20114f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7794117647058824\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81        40\n",
      "           1       0.72      0.75      0.74        28\n",
      "\n",
      "    accuracy                           0.78        68\n",
      "   macro avg       0.77      0.78      0.77        68\n",
      "weighted avg       0.78      0.78      0.78        68\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[32  8]\n",
      " [ 7 21]]\n"
     ]
    }
   ],
   "source": [
    "# Initialise Random Forest Classifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af85fbc9-4323-44e7-b09b-3a74c53c2ac7",
   "metadata": {},
   "source": [
    "## Step 5: Deep Learning (Neural Network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6e3fb89-3376-4232-b0cb-a6885f25bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cost = 0.6932587802033876\n",
      "Epoch 100: Cost = 0.6914967960997489\n",
      "Epoch 200: Cost = 0.6903402182382528\n",
      "Epoch 300: Cost = 0.6894335940318848\n",
      "Epoch 400: Cost = 0.6884516577544201\n",
      "Epoch 500: Cost = 0.6870121901438209\n",
      "Epoch 600: Cost = 0.6844815019067907\n",
      "Epoch 700: Cost = 0.6797770999905648\n",
      "Epoch 800: Cost = 0.6709942576786946\n",
      "Epoch 900: Cost = 0.6553585075390747\n",
      "Epoch 1000: Cost = 0.6296226929244526\n",
      "Epoch 1100: Cost = 0.5920722623094518\n",
      "Epoch 1200: Cost = 0.5444855722496802\n",
      "Epoch 1300: Cost = 0.49178207770905524\n",
      "Epoch 1400: Cost = 0.4406239389042683\n",
      "Epoch 1500: Cost = 0.3980573055579895\n",
      "Epoch 1600: Cost = 0.367279442488699\n",
      "Epoch 1700: Cost = 0.34674618644895866\n",
      "Epoch 1800: Cost = 0.3332106027519903\n",
      "Epoch 1900: Cost = 0.32405643363813785\n",
      "Neural Network Accuracy: 0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "# Initialise Deep Learning\n",
    "# Initialise parameters \n",
    "def initialise_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    parameters = {\n",
    "        \"W1\": np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        \"b1\": np.zeros((hidden_size, 1)),\n",
    "        \"W2\": np.random.randn(output_size, hidden_size) * 0.01,\n",
    "        \"b2\": np.zeros((output_size, 1))\n",
    "    }\n",
    "    return parameters\n",
    "#Defining Activation Functions\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return (Z > 0).astype(int)\n",
    "#Forward Propagation\n",
    "def forward_propagation(X, parameters):\n",
    "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "#Computing the Cost\n",
    "def compute_cost(Y, A2):\n",
    "    m = Y.shape[1]\n",
    "    cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n",
    "    return np.squeeze(cost)\n",
    "#Backpropagation (updating parameters during training)\n",
    "def backward_propagation(X, Y, parameters, cache):\n",
    "    m = X.shape[1]\n",
    "    W2 = parameters[\"W2\"]\n",
    "\n",
    "    dZ2 = cache[\"A2\"] - Y\n",
    "    dW2 = np.dot(dZ2, cache[\"A1\"].T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "\n",
    "    dZ1 = np.dot(W2.T, dZ2) * relu_derivative(cache[\"Z1\"])\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return grads\n",
    "#Updating Parameters\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    for key in parameters.keys():\n",
    "        parameters[key] -= learning_rate * grads[\"d\" + key]\n",
    "    return parameters\n",
    "\n",
    "#training \n",
    "def train_neural_network(X, Y, input_size, hidden_size, output_size, epochs=1000, learning_rate=0.01):\n",
    "    parameters = initialise_parameters(input_size, hidden_size, output_size)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(Y, A2)\n",
    "        grads = backward_propagation(X, Y, parameters, cache)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {i}: Cost = {cost}\")\n",
    "\n",
    "    return parameters\n",
    "#Making Predictions\n",
    "def predict(X, parameters):\n",
    "    A2, _ = forward_propagation(X, parameters)\n",
    "    return (A2 > 0.5).astype(int)\n",
    "#test\n",
    "input_size = X_train_nn.shape[0]\n",
    "trained_parameters = train_neural_network(\n",
    "    X_train_nn,\n",
    "    y_train_nn,\n",
    "    input_size=input_size,\n",
    "    hidden_size=8,\n",
    "    output_size=1,\n",
    "    epochs=2000,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "predictions = predict(X_test_nn, trained_parameters)\n",
    "\n",
    "accuracyNN = np.mean(predictions == y_test_nn)\n",
    "print(\"Neural Network Accuracy:\", accuracyNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad839068-3ed8-4bf0-bf7b-1c9ed1e94741",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Step 6: Make Risk Score Predictions with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16bd65b5-eca7-45dc-b495-450be6f712a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'KNN Prediction': 'Absence', 'Random Forest Prediction': 'Absence', 'Neural Network Prediction': 'Absence', 'Final Risk Level': 'Low Risk'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natas\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\natas\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialise Rise Score, get input data from app\n",
    "def ensemble_risk_prediction(input_data):\n",
    "    scaled = scaler.transform([input_data])\n",
    "\n",
    "    # Individual model predictions\n",
    "    knn_pred = knn.predict(scaled)[0]\n",
    "    rf_pred = model.predict(scaled)[0]\n",
    "\n",
    "    nn_input = scaled.T\n",
    "    nn_pred = predict(nn_input, trained_parameters)[0][0]\n",
    "\n",
    "    # Convert numeric prediction back to label\n",
    "    knn_label = le.inverse_transform([knn_pred])[0]\n",
    "    rf_label = le.inverse_transform([rf_pred])[0]\n",
    "    nn_label = le.inverse_transform([nn_pred])[0]\n",
    "\n",
    "    votes = knn_pred + rf_pred + nn_pred\n",
    "\n",
    "    if votes == 0:\n",
    "        risk = \"Low Risk\"\n",
    "    elif votes == 3:\n",
    "        risk = \"High Risk\"\n",
    "    else:\n",
    "        risk = \"Medium Risk\"\n",
    "\n",
    "    return {\n",
    "        \"KNN Prediction\": knn_label,\n",
    "        \"Random Forest Prediction\": rf_label,\n",
    "        \"Neural Network Prediction\": nn_label,\n",
    "        \"Final Risk Level\": risk\n",
    "    }\n",
    "#testing\n",
    "patient = [20, 1, 4, 170, 199, 1, 2, 160, 1, 2.3, 1, 0, 3]\n",
    "#now I need to add the data from app\n",
    "\n",
    "print(ensemble_risk_prediction(patient))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e612257-c74c-42c8-b8e5-1db6deedd749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
